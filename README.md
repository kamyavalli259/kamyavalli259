# Hi, I'm Kamyavalli ğŸ‘‹

ğŸ“ **Computer Science Graduate (Aug 2025)**  
ğŸ’» Aspiring **Software Engineer** with experience in **full-stack development, Python, and data analytics**  
ğŸŒ± Actively improving **Data Structures & Algorithms** and backend development skills

---

## ğŸ”§ Technical Skills
- **Languages:** Python, Java, JavaScript, SQL  
- **Data Engineering & Big Data:** PySpark, Apache Spark, ETL Pipelines, Automated Data Ingestion, Scheduling
(Cron, APScheduler) 
- **Machine Learning & Statistics:** Scikit-Learn, XGBoost, Time Series Forecasting (ARIMA/SARIMAX), Statistical
Modeling
- **Cloud & Storage:** Amazon S3, Cloud Object Storage, Data Partitioning, Data Lifecycle Management
- **MLOps & Reliability:** MLflow, Experiment Tracking, Logging, Monitoring, Fault-Tolerant Pipelines
- **Tools:** Git, GitHub, VS Code, Google Colab  

---

## â­ Featured Projects

### ğŸ² Recipe Finder Application
**Node.js, Express, MongoDB**
- Built a full-stack web application to search recipes by cuisine and food items
- Implemented user ratings and feedback stored in MongoDB
- Designed RESTful APIs and dynamic UI using EJS templates

### ğŸ“Š Digital Manufacturing Demand Forecasting
**Python, PySpark, XGBoost, ARIMA, MLflow, Linear Programming**
- Built an end-to-end demand forecasting and capacity optimization pipeline using PySpark, Python, and XGBoost,
driving significant improvements in simulated on-time delivery and capacity utilization.

- Designed scalable Spark ETL and feature pipelines for time-series demand forecasting across multiple product
categories and regions.

- Implemented optimization-based manufacturer allocation using linear programming to reduce capacity bottlenecks.
- Conducted model experimentation and tracking using MLflow to compare statistical and ML-based forecasts.
- Created synthetic datasets to enable reproducible, large-scale pipeline testing.

- Created synthetic datasets to enable reproducible, large-scale pipeline testing.
  

###  Automated Data Ingestion and Scheduling Pipeline 
**Python, REST APIs, Amazon S3, Boto3, Automated Scheduling, ETL Pipelines, Logging**
- Built a production-style Python pipeline to ingest external API data on an hourly schedule with schema validation
and fault tolerance.
- Designed modular storage layers supporting both local file systems and cloud-based object storage (Amazon S3).
- Implemented data retention and archival policies to manage hot, warm, and cold data storage.
- Developed configurable scheduling using cron/APScheduler to support automated and backfill ingestion.
- Added structured logging and basic observability for monitoring pipeline health and failures.

ğŸ‘‰ Check out my pinned repositories for more details!

---

## ğŸŒ± Currently Learning
- Advanced Data Structures & Algorithms
- Backend system design
- Interview-focused coding problems

---

## ğŸ“« Connect With Me
- **LinkedIn:** https://www.linkedin.com/in/kamyavallimopidevi259/
- **Email:** kamyavalli.mopidevi@gmail.com

---

âœ¨ I enjoy building practical projects, learning new technologies, and growing as a software engineer.
